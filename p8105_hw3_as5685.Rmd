---
title: "p8105_hw3_as5685"
output: github_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(p8105.datasets)
```

## Load in "instatcart" dataset
```{r}
data("instacart")
nrow(instacart)
ncol(instacart)
```
The size of table is `r nrow(instacart)` and `r ncol(instacart)`. 
Each row in the dataset is a product from an order, and there is a single order per user. 
`product_id` is the product identifier, each id is correspondent to unique product. For example, if we have the id "49302", we will know it is Bulgarian Yogurt in Yougurt aisle. 
`product_name` and `aisle` indicate the name of specific product and which aisle it is placed in the grocery  which make easier for sorting them. such as Fresh Dill belongs to fresh herbs. 
Some other variables such as `order_dow`. `order_hour_od_day` and `reordered` are helpful for staff to do some market research. 

## Number of aisles and items ordered
```{r}
aisles_data = 
instacart %>% 
  group_by(aisle) %>% 
  summarize(
    aisle_number = n()
  ) %>% 
  filter(min_rank(desc(aisle_number)) < 2) %>% 
  knitr::kable()
```
The number of aisles are `r nrow(aisles_data)`. 
Fresh vegatables is the aisle that most item ordered from. 

## Plot for number of items ordered
```{r}
popularitem_plot = 
    instacart %>% 
      group_by(aisle) %>% 
      summarize(
      order_amount = n()
      ) %>% 
    filter(order_amount > 10000) %>% 
    ggplot(aes(y = aisle, x = order_amount)) +
      geom_point() +
      labs(
        title = "number of items ordered from aisles with more than 10000 items",
        y = "Aisle Name",
        x = "Order Amount",
        caption = "Data from Instacart"
      )
```

## Table showing three most popular items in each aisle 
```{r}
popularitem_data =
  instacart %>% 
   group_by(aisle, product_name) %>% 
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
  summarize(
    product_order_amount = n()
  ) %>% 
  filter(min_rank(desc(product_order_amount)) < 4) %>% 
  arrange(desc(product_order_amount)) %>% 
  knitr::kable()
popularitem_data 
```

The most popular three items in `baking ingredients` are `Cane Sugar` with 336 ordered, `Light Brown Sugar` with 499 ordered and `Pure Baking Soda` with 387 ordered. 
The most popular three items in `dog food care` are `rganix Chicken & Brown Rice Recipe` with 28 ordered `Small dog biscuits`with  `Snack Sticks Chicken & Rice Recipe Dog Treats`


## Mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week
```{r}
meanhours_data = 
instacart %>% 
  group_by(product_name,order_dow) %>% 
    filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
  summarize(meanhours = mean(order_hour_of_day)) %>% 
  pivot_wider(
    names_from = "order_dow",
    values_from = "meanhours"
  ) %>% 
knitr::kable(digits = 1) 
meanhours_data
```

## Problem 2 
## load and read in data "BRFSS"
```{r}
data("brfss_smart2010")
```

## data cleaning 
```{r}
brfss_dataclean = 
  brfss_smart2010 %>% 
  janitor::clean_names() %>% 
  filter(topic == "Overall Health") %>% 
  filter(response %in% c("Excellent","Fair","Good","Poor","Very good")) %>% 
  mutate(
    response = factor(response, levels = c("Poor","Fair","Good","Very good","Excellent")) 
    ) 
```

## states obseved at 7 or more.
```{r}
location_count_2002 = 
brfss_dataclean %>% 
   filter(year %in% c("2002","2010")) %>% 
    group_by(year, locationabbr) %>%
    summarize(
      state_n = n(),
      location_n = n_distinct(locationdesc)
    ) %>% 
  mutate(greatherthan7 = location_n >= 7) %>% 
  filter(greatherthan7 == "TRUE")
```
In 2002, "NJ", "PA","NC", "MA", "FL","CT" is the state obeserved at 7 locations. In 2010, "CA","FL","CO","MA","MD","NC","NE","NJ","NY","OH","PA", "SC","TX","WA" are the states observed at 7 or more locations. 


## spaghetti plot
```{r}
excellent_datasets = 
  brfss_dataclean %>%
  group_by(locationabbr,year) %>% 
  filter(response == "Excellent") %>% 
  mutate(
    datavalue_mean = mean(data_value, na.rm = TRUE)
  ) %>% 
  select(year, locationabbr, datavalue_mean,response) %>% 
  distinct()

spaghetti_plot = 
  ggplot(data = excellent_datasets, aes(x = year, y = datavalue_mean, color = locationabbr)) +
  geom_line()
```

## two-panel plot
```{r}
plot = 
  brfss_dataclean %>% 
  filter(year %in% c("2006","2010"), locationabbr == "NY") %>% 
  select(response,year,locationdesc,data_value) %>% 
  ggplot(aes(x = data_value, fill = response)) + 
    geom_density() +
    facet_grid(.~locationdesc) 
    #viridis::scale_fill_viridis(discrete = TRUE)
```
## problem 3

## load, tidy and wrangle the data
```{r}
CHF_data = 
  read_csv("./p8105_hw3_as5685_files/accel_data.csv") %>% 
  janitor::clean_names() %>% 
  
  mutate(
    day = factor(day, levels = c("Monday","Tuesday","Wednesday","Thursday","Friday","Saturday", "Sunday")),
    weekday = day %in% c("Monday","Tuesday","Wednesday","Thursday","Friday") 
    ) %>% 
    
    select(week,day_id,day,weekday,everything()) %>% 
      
  pivot_longer(
    activity_1:activity_1440, 
    names_to = "activity_num",
    values_to = "count_for_minute")
```
I used `pivot_longer` to organize the table. The variables are `week`, `day_id`, `day`, `weekday`, `activity number` and `count_for_minute`

## aggregate accross minutes to create a total activity variable for each day, and create a table showing these totals.
```{r}
totalactivity_data = 
  CHF_data %>% 
  group_by(day_id) %>% 
  mutate(
   day_total_activity = sum(count_for_minute)
  ) %>% 
  select(day_id,day_total_activity) %>% 
  distinct()
  knitr::kable
```

## single-panel plot
```{r}
activity_plot = 
  CHF_data %>% 
  ggplot(aes(x = week, y = count_for_minute, color = day)) +
  geom_point() 
```










